{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954f1f94",
   "metadata": {},
   "source": [
    "# Jupyter Notebook: Comparing Word Embeddings - CBOW, Skip-Gram, FastText\n",
    "\n",
    "# ðŸ“˜ Title\n",
    "# Comparing Word Embedding Models: CBOW, Skip-Gram, and FastText\n",
    "\n",
    "# In this notebook, we'll train and visualize three popular word embedding models using the Brown corpus:\n",
    "# - Word2Vec with CBOW\n",
    "# - Word2Vec with Skip-Gram\n",
    "# - FastText\n",
    "\n",
    "# We'll also use PCA to reduce the embeddings to 2D space for easy visualization.\n",
    "\n",
    "# ------------------------------\n",
    "# ðŸ“¦ Import Dependencies and Load Data\n",
    "# ------------------------------\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "# Make sure the Brown corpus is available\n",
    "nltk.download('brown')\n",
    "sentences = brown.sents()\n",
    "\n",
    "# ------------------------------\n",
    "# ðŸ§  Step 1: Train the Embedding Models\n",
    "# ------------------------------\n",
    "\n",
    "# We'll train:\n",
    "# - A Word2Vec model using CBOW (sg=0)\n",
    "# - A Word2Vec model using Skip-Gram (sg=1)\n",
    "# - A FastText model, which includes subword information\n",
    "\n",
    "def train_models(sentences):\n",
    "    print(\"Training Word2Vec (CBOW)...\")\n",
    "    cbow = Word2Vec(sentences, vector_size=100, window=5, min_count=2, sg=0)\n",
    "\n",
    "    print(\"Training Word2Vec (Skip-Gram)...\")\n",
    "    skipgram = Word2Vec(sentences, vector_size=100, window=5, min_count=2, sg=1)\n",
    "\n",
    "    print(\"Training FastText...\")\n",
    "    fasttext = FastText(sentences, vector_size=100, window=5, min_count=2)\n",
    "\n",
    "    return cbow, skipgram, fasttext\n",
    "\n",
    "# ------------------------------\n",
    "# ðŸ“Š Step 2: Visualize Word Embeddings\n",
    "# ------------------------------\n",
    "\n",
    "# We'll use PCA to reduce the 100-dimensional vectors to 2D, so we can plot them.\n",
    "# This helps us get a sense of how the models learn relationships between words.\n",
    "\n",
    "def visualize_embeddings(models, words, titles):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        word_vectors = []\n",
    "        labels = []\n",
    "\n",
    "        for word in words:\n",
    "            if word in model:\n",
    "                word_vectors.append(model[word])\n",
    "                labels.append(word)\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced = pca.fit_transform(word_vectors)\n",
    "\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.scatter(reduced[:, 0], reduced[:, 1], edgecolors='k')\n",
    "\n",
    "        for label, x, y in zip(labels, reduced[:, 0], reduced[:, 1]):\n",
    "            plt.annotate(label, (x, y))\n",
    "\n",
    "        plt.title(titles[i])\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# ðŸš€ Step 3: Run Everything\n",
    "# ------------------------------\n",
    "\n",
    "# Now weâ€™ll:\n",
    "# - Train all three models\n",
    "# - Pick some target words\n",
    "# - Visualize how each model positions those words in vector space\n",
    "\n",
    "# Train the models\n",
    "cbow_model, skipgram_model, fasttext_model = train_models(sentences)\n",
    "\n",
    "# Define a list of words we want to visualize\n",
    "target_words = ['king', 'queen', 'man', 'woman', 'dog', 'cat', 'car', 'road', 'city', 'village']\n",
    "\n",
    "# Wrap models into a dictionary for easy visualization\n",
    "models = {\n",
    "    'CBOW': cbow_model.wv,\n",
    "    'Skip-Gram': skipgram_model.wv,\n",
    "    'FastText': fasttext_model.wv,\n",
    "}\n",
    "\n",
    "# Titles for plots\n",
    "titles = ['Word2Vec CBOW', 'Word2Vec Skip-Gram', 'FastText']\n",
    "\n",
    "# Visualize the word embeddings\n",
    "visualize_embeddings(models, target_words, titles)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
